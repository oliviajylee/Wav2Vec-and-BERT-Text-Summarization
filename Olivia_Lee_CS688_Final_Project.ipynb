{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 688 Project: Sound to Text and Text Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import librosa\n",
    "import editdistance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound to Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/Users/OliviaLee/Library/Python/3.9/lib/python/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:757: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9tghzd8j685bj8tt_zfx89z80000gn/T/ipykernel_51823/2952788319.py:2: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  total_duration, segment_length, start_at = librosa.get_duration(filename=fname), 20, 0\n"
     ]
    }
   ],
   "source": [
    "fname = \"TED_Talk.wav\"; duration = 15.6\n",
    "total_duration, segment_length, start_at = librosa.get_duration(filename=fname), 20, 0\n",
    "segment_start_times = range(start_at, int(total_duration), segment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcript: 165\n"
     ]
    }
   ],
   "source": [
    "sentences = actual_transcript.split(\".\")\n",
    "sentences = [s for s in sentences if s]  # remove empty strings\n",
    "sentences = [s.split(\"!\") for s in sentences]\n",
    "sentences = [item for sublist in sentences for item in sublist]  # flatten list of lists\n",
    "sentences = [s.split(\"?\") for s in sentences]\n",
    "sentences = [item for sublist in sentences for item in sublist]  # flatten list of lists\n",
    "num_sentences = len(sentences)\n",
    "print(\"Number of sentences in transcript:\",num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TED_Talk_Transcript.txt\", \"r\") as f:\n",
    "    actual_transcript = f.read().strip()\n",
    "\n",
    "actual_transcript = actual_transcript.replace('/', '').upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 47\n"
     ]
    }
   ],
   "source": [
    "num_segments = len(segment_start_times)\n",
    "print(\"Number of segments:\", num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing segment 1/47\n",
      "Processing segment 2/47\n",
      "Processing segment 3/47\n",
      "Processing segment 4/47\n",
      "Processing segment 5/47\n",
      "Processing segment 6/47\n",
      "Processing segment 7/47\n",
      "Processing segment 8/47\n",
      "Processing segment 9/47\n",
      "Processing segment 10/47\n",
      "Processing segment 11/47\n",
      "Processing segment 12/47\n",
      "Processing segment 13/47\n",
      "Processing segment 14/47\n",
      "Processing segment 15/47\n",
      "Processing segment 16/47\n",
      "Processing segment 17/47\n",
      "Processing segment 18/47\n",
      "Processing segment 19/47\n",
      "Processing segment 20/47\n",
      "Processing segment 21/47\n",
      "Processing segment 22/47\n",
      "Processing segment 23/47\n",
      "Processing segment 24/47\n",
      "Processing segment 25/47\n",
      "Processing segment 26/47\n",
      "Processing segment 27/47\n",
      "Processing segment 28/47\n",
      "Processing segment 29/47\n",
      "Processing segment 30/47\n",
      "Processing segment 31/47\n",
      "Processing segment 32/47\n",
      "Processing segment 33/47\n",
      "Processing segment 34/47\n",
      "Processing segment 35/47\n",
      "Processing segment 36/47\n",
      "Processing segment 37/47\n",
      "Processing segment 38/47\n",
      "Processing segment 39/47\n",
      "Processing segment 40/47\n",
      "Processing segment 41/47\n",
      "Processing segment 42/47\n",
      "Processing segment 43/47\n",
      "Processing segment 44/47\n",
      "Processing segment 45/47\n",
      "Processing segment 46/47\n",
      "Processing segment 47/47\n"
     ]
    }
   ],
   "source": [
    "combined_transcription = \"\"\n",
    "for i, start_time in enumerate(segment_start_times):\n",
    "\n",
    "    print(f\"Processing segment {i+1}/{num_segments}\")\n",
    "\n",
    "    start_time = i * segment_length + start_at\n",
    "    end_time = start_time + segment_length\n",
    "\n",
    "    # Load \"segment_length\" seconds of the file, starting at \"start_at\" seconds\n",
    "    speech, rate = librosa.load(fname, sr=16000, offset=start_time, duration=segment_length)\n",
    "\n",
    "    # Tokenize the waveform\n",
    "    input_values = tokenizer(speech, return_tensors='pt').input_values\n",
    "\n",
    "    # Retrieve logits from the model\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "    # Take argmax value and decode into transcription\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)\n",
    "\n",
    "    # Combine the transcriptions\n",
    "    combined_transcription += \" \" + transcription[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted transcribed audio:  SO ANYONE WHO'S BEEN PAYING ATTENTION FOR THE LAST FEW MONTHS HAS BEEN SEEING HEADLINES LIKE THIS ESPECIALLY IN EDUCATION THE THESIS HAS BEEN STUDENTS ARE GOING TO BE USING CHAT G P T IN OTHER FORMS OF A I TO CHEAT DU THEIR ASIGNMENTS THEY 'RE NOT GOING TO LEARN ANS GOING TO COMPLETELY UNDERMINE EDUCATION AS WE KNOW IT NOW WHAT I'M AN ARGUE TODAY IS NOT ONLY ARE THEIR WAYS TO MITIGATE ALL THE FACT IF WE PUT THE RIGHT GADRAILS WE DO THE RIGHT THINGS WE CAN MITIGATE IT BUT I THING GERE AT THE COSP OF USING A I FOR PROBABLY THE BIGGEST TRANSPOSITIVE TRANSFORMATION THAT EDUCATION HAS EVER SEEN AND THE WAY WE'RE GOING TO DO THAT IS BY GIVING EVERY STUDENT ON THE PLANET AN ARTIFICIALLY INTELLIGENT BUT AMAZING PERSONAL TUTOR AND ARE GOING TO GIVE EVERY TEACHER ON THE PLANET A AN AMAZING ARTIFICIALLY INTELLIGENT TEACHING ASSISTANT AND JUST TO APPRECIATE HOW BIG OF A DEAL IT WOULD BE TO GIVE EVERY ONE A PERSONAL TUTOR I SHOW YOU THIS CLIP FROM BENJAMIN BLOOMS NINETEEN EIGHTY FOUR TO SIGMA STUDY OR HE CALLED IT THE TWO SIGMA PROBLEM THE TWO SIGMA COMES FROM TWO STANDARD DEVIATIONS SIGMA THE SYMBOL FOR STANARDEVIATION AND HE HAD GOOD DATA THAT SHOWED THAT LOOK THE NORMAL DISTRIBUTION THAT'S THE ONE THAT YOU SEE IN T THE TRADITIONAL BELCURVE RINE IN THE MIDDLE THAT'S HOW NO THE WORLD CAN AFSORTS ITSELF OUT THAT IF YOU WERE TO GIVE PERSONAL ONE TO ONE TO TUTORING FOR STUDENTS THAT YOU CAN ACTUALLY GET A DISTRIBUTION THAT LOOKS LIKE THAT RIGHT IT SAYS TUTORIAL ONE TO ONE WITH THE ASTRICTS LIKE THAT RIGHT DISTRIBUTION A TWO STANDARD DEVIATION IMPROVEMENT JUST TO PUT THAT IN PLA AIN LANGUAGE THAT COULD TAKE YOUR AVERAGE STUDENT AND TURN THEM INTO AN EXCEPTIONAL STUDENT IT CAN TAKE YOUR BELOW AVERAGE STUDENT AND TURN THEM INTO AN ABOVE AVERAGE STUDENT NOW THE REASON WHY HE FRAMED IT AS A PROBLEM WAS HE SO WELL THIS IS ALL GOOD BUT HOW DO YOU ACTUOU SCALE GROUP INSTRUCTION THIS WAY HOW YOU ACTUALLY GIVE IT TO EVERY ONE IN ECONOMIC WAY WHAT I'M ABOUT TO SHOW YOU AS I THINK THE FIRST MOVE TOWARDS DOING THAT OBVIOUSLY 'VE BEEN TRYING TO APPROXIMATE IT IN SOME WAY AT CONACADEMY FOR OVER A DECADE NOW BUT I THINK WE'RE AT THE CUSP OF ACCELERATING IT DRAMATICALLY IM WOING TO SHOW YOU THE EARLY STAGES OF WHAT AR A I WHICH WE CALL KONMIGO WHAT IT CAN NOW DO AND MAY BE A LITTLE BIT OF WHERE IT IS ACTUALLY GOING SO THIS RIGHT OVER HEREIS A TRADITIONAL EXERCISE THAT YOU OR MANY OF YOUR CHILDREN MIGHT HAVE SEEN ON CON ACADEMY BUT WHAT'S NEW IS A THAT LITTLE LITTLE BOTING AT THE RIGHT AND WILL START BY SEEING ONE OF THE VERY IMPORTANT SAFEGUARDS WHICH IS CONVERSATION IS RECORDED AND VIEWABLE BY YOUR TEACHER ITS MODERATED ACTUALLY BY A SECOND AY EYE AND ALSO DOES NOT TELL YOU THE ANSWER IT IS NOT A CHEATING TOOL NOTICE WHEN THE STUDENTSAYS TELL ME ANSWER SAYS IM TUTER WHAT YOU THINK IF THE NEXT STEP FOR SOLVING THE PROBLEM NOW IF THE STUDENT MAKES THE MISTAKE AND THIS WILL SURPRISE PEOPLE WHO THINK LARGE LANGUAGE MODELS ARE NOT GOOD AT MATHEMATICS NOTICE NOT ONLY DOES IT NOTICE THE MISTAKE IT ASKD THE STUDENT TO EXPLAIN THEIR REASONING BUT IT'S ACTUALLY DOING WHAT I WOULD SAY NOT JUST EVEN AN AVERAGE TUTOR WOULD DO BUT AN EXCELLENT TUTOR WOULD DO IT'S ACTUALLY IT'S ABLE TO DIVINE WHAT IS PROBABLY THE MISCONCEPTION IN THAT STUDENT'S MIND THAT THEY PROBABLY DIDN'T USE THE DISTRIBUTE IF PROPERLY REMEMBER WE NEDO DISTRIBUTE THE NEGA IF TWO TO BOTH THE NINE AND THE TO M INSIDE OF THE PARENTHISES THIS TO ME IS A VERY VERY VERY BIG DEAL AND IT'S NOT JUST IN MATH THIS IS A COMPETER PROGRAMMING EXERCISE ON ACON ACADEMY WHERE THE STUDENT NEEDS TO MAKE ATHE CLOUDS PART AND SO WE CAN SEE THE STUDENT STARTS TO FINDING A VERIABLE LEFT X MIT AS MIT AS IT ONLY MADE THE LEFT CLOUD PART BUR THEN THEY CAN ASK CON MIGO WHAT'S GOING ON WISE ONLY THE LEFT CLOUD MOVING AND IT UNDERSTANDS THE CODE IT KNOWS ALL THE CONTECTS OF WHAT THE STUDENT IS DOING AND IT UNDERSTANDS THAT THOSE ELLIPSES ARE THERE TO DRAW CLOUDS WHICH I THINK IS KIND OF MIND BLOWING AND IT SAYS TO MAKE THE RIGHT CLOUD MOOD AS WELL TRY ADDING LINE OF COD INSIDE THE DRAW FUNCTION THAT INCREMENTS THE RIGHT EX VERIBLE BY ONE PIXLE IN EACH FRAME NOW TH THIS ONE IS MAYBE EVEN MORE AMAZING BECAUSE WE HAVE A LOT OF MATH TEACHERS WE'VE ALL BEEN TRYING TO TEACH THE WORLD TO CODE BUT THERE AREN'T A LOT OF COMPUTING TEACHERS OUT THERE AND WHAT YOU JUST SAW EVEN WHEN I'M TUTORING MY KIDS WHEN THEY'RE LEARNING TO CODE I CAN'T HELP EM THIS W ELL THIS FAST THIS IS REALLY GOING TO BE A SUPER TUTOR AND IT'S NOT JUST EXERCISE IS IT UNDERSTANDS WHAT YOUR WATCHING IT UNDERSTANDS THE CONTIXT OF YOUR VIDIO IT CAN ANSWER THE AGE OLD QUESTION WHY DO I NEED TO LEARN THIS AND IT ASTOCRATICALLY WELL WHAT DO YOU CARE ABOUT AND LET'S SAY THE STUDENT SAYS I WANT TO BE A PROFESSIONAL A FLEET AND IS AS WELL LEARNING ABOUT THE SIZE OF SELLS WHICH IS WHAT THIS VIDIOS ARE THAT COULD BE REALLY USEFUL FOR UNDERSTANDING NUTRITIAN AND HOW YOUR BODY WORKS ET CETERA IT CAN ANSWER QUESTIONS IT CAN QUIZU IT CAN CONNECT IT TO OTHER IDEAS YOU CAN NOW ASK AS MANY QUESTIONS OF VIDIO AS AS YOU COULD EVER DREAM OF THO ANOTHER BIG SHORTAGE OUT THERE I REMEMBER THE HIGH SCHOOL I WENT TO THE STUDENT TO GUIDANCE CONCILORATIO WAS ABOUT TWO HUNDRED OR THREE HUNDRED TO ONE A LOT OF THE COUNTRY IT'S WORSE THAN THAT WE CAN USE CONMUGO TO GIVE EVERY STUDENT A GUIDANCE COUNCILLOR ACADEMIC COACH CAREER COACH LIFE COACH WHICH IS EXACTLY WHAT YOU SEE RIGHT OVER HERE AND THIS IS ALL YOU KNOW WE LAUNCH THIS WITH THE GIPITY FOR LAUNCH WE HAVE A FEW THOUSAND PEOPLE ON THIS THIS ISN'T A FAKE DANMODIS THIS IS REALLY IT IN INACTION AND THEN THEREIS YOU KNOW THINGS THAT I THINK IT WOUL HAVE BEEN EVEN HARD IUDHAE BEN LITTLE SINSE FICTION DO WITH EVEN A TRADITIONAL TUTOR THEREIS A STUDENT WE RUN AN ONLINE HIGH SCHOOL WITH ARIZONA STATE UNIVERSITY CALLE CONWORLD SCHOOL AND WE HAVE A STUDENT WHO TENDS THAT ONLINE SCHOOL AT BASIN INTO HER NAME SANVI AND SHE WAS DOING A REPORT ON THE GREAT GASPI AND WHEN SHE WAS READING THE GREAT GASPE J GASPY KEEPS LOOKING AT THE GREEN LIGHT OFF INTO THE DISTANCE AND SHE'S A GOAD HE DO THAT SHE DID SOME WEB SEARCHES AND PEOPLE OF OBVIOUSLY STUDY THIS AND COMMENTED ABOUT THE SYMBOLISM OF THAT BUT NONE OF IT WAS REALLY RESINATING WITH HER AND THEN SHE REALIZED THAT SHE HAD CONMIGO AND THAT SHE COULD TALK TO J GATSBY HIMSELF AND SO AH SPLENDID CHOICE SOL SPORT I AM NOW J GATSBY THE ENIGMATIC MILLIONAIRE FROM EFGOTTS GEROLSCLASS IC AN SO WHY DO YOU KEEP STARING AT THE GREEN LIGHT AH THE GREEN LIGHT OLD SPORT IT'S THE SYMBOL OF MY DREAMS AND DESIRES YOU SEE IT'S SITUATED AT THE END OF DAISY BUCHENNAN'S DOCK ACROSS THE BAY FOR MY MANSION I GAZE AT IT LONGINGLY AS IT REPRESENTS MY YEARNING FOR THE PAST AND MY HOPE TO REUNITE WITH DAISY THE LOVE OF MY LIFE AND WHAT W AS COOL AS SAI HAD SAID I HAD THIS LONG CONVERSATION SHE CALLED I MITZER MISTER GATSBY AND A THE ENSHIATRE APOLOGIZED FOR TAKING HIS TIME WHICH I THOUGHT WAS VERY POLITE OF HER BUT YOU CAN IMAGINE THIS UNLOCKS LEARNING LITERATURE LEARNING A HISTRY CD TALK TO HISTORICAL FIGURES OR EVEN PROBE GIN TO AD AN ACTIVITY AN TALK TO LIKE THE MISSISSIPPI RIVER IT BRINGS THINGS TO LIFE AND WAGS THAT YOU THAT REALLY WERE SCIENCE FICTION EVEN SIX MONTHS OR OR A YEAR AGO STUDENTS CAN GET INTO DEBATES WITH THE A I AND WE'VE GOT THE HEARS O STUDENT'S DEBATING WHETHER WE SHOULD CANCIL STUDENT DEBT THE STUDENT IS AGAINST CANCILLING STUDENT DEBT AND WE'VE GOTTEN VERY CLEAR FEED ACT WE STARTED RUNNING IT AT CONROREL SCHOOL IN OUR LAB SCHOOL OF THAT WE HAVE CONRADE SCHOOL THE STUDENTS THE HIGH SCHOOL SONS ESPECIALLY THEIR SAGNIS IS AMAZING TO BE LEFINE TOTH IN MY ARGUMENTS WITHOUT FEARING JUDGMENT IT MAKES ME THAT MUCH MORE CONFIDENT TO KIND OF GO INTO THE CLASS ROOM AND REALLY PARTICIPATE AND WE ALL KNOW THAT SOCRATIC DIALOGUE DEBATE IS ONE A GREAT WAY TO LEARN BUT FRANKLY 'S NUTS NOT OUT THERE FOR MOMOSTUDENTS BUT NOW IT CAN BE ACCESSIBLE TO HOPEFULLY EVERYONE A LOT OF THE NARRATIVE WE SAW THAT IN THE HEADLINES HAS BEEN IT'SGOING TO DO THE WRITING FOR KIDS KIDS ARE NOT GOING TO LEARN TO WRITE BUT WE ARE SHOWING THAT THERE'S WAYS THAT THE AID DOESN'T WRITE FOR YOU IT WRITES WITH YOU SO THIS IS A A LITTLE THING AND MY EIGHT YEAR OLD AS A DIXIDUS AND HE'S NOT A KID THAT REALLY LIKED WRITING BEFORE BUT WHERE A UNICAUS I WANT WRITE A HORROR STORY AND IT SAYS OO A HORROR STORY HOUSE FINE TINGLING AND THRILLING LET'S DIVE INTO THE WORLD OF ERIE SHADOWS AND CHILLING MYSTERIES AND THIS IS AN ACTIVITY WHERE THE STUDENT WILL WRITE TWO SENTENCES AND THEN THE A I WILL WRITE TWO SENTENCES AND SO THEY COLLABORATE TOGETHER A HONOST STORY THE SOOTRITE BEATRICE WAS A MISUNDERSTOOD GHOST SHE WANTED TO MAKE FRIENDS BUT KEPT SCARING THEM BY ACCIDENT AND THE A I SAYS POOR BEATRICE A LONELY SPIRIT YEARNING FOR COMPANIONSHIP ONE DAY SHE STUMBLED UPON AN OLD ABANDON MANSION ET CETERA ET CETERA I ENCOURAGE ALTO YOU KNOW HOPEFULLY ONE DAY TRY THIS THIS IS SURPRISINGLY FUN NOW TO EVEN MORE DIRECTLY AHIT THIS USECASE AND WHAT IAM ABOUT TO SHOW YOU EVERYTHING I SHOWED YOU SO FAR AS ACTLLY ALL PART ALREADY PART OF CANMIGO AT I 'M ABOUT TO SHOW YOU WE HAVEN'T SHOWN TO ANY ONE YET OW THIS IS A PROTOTYPE WE HOPE TO BE ABLE LAUNCH IT IN THE NEXT FEW MONTHS BUT THIS IS TO DIRECTLY USE A I USE GENERATIVE A I TO NOT UNDER MINE ENGLISH AND LANGUAGE ARTS BUT TO ACTUALLY ENHANCE IT IN WAYS THAT WE COULDN'T HAVE EVEN CONCEIVED OF EVEN A YEAR AGO THIS IS READING COMPREHENSION THE STUDENTS READING STIVE JOSPHEMOUS A A SPEECH AT STAMFORD AND THEN AS THEY GET TO CERTAIN POINTS THEY CAN CLICK ON THAT LITTLE QUESTION AND THE A I WILL THEN ND SOCRATICALLY ALMOST LIKE AN ORAL EXAM ASK THE STUDENT ABOUT THINGS AND THE A I CAN HILIGHT PARTS OF THE PASSAGE WHY DID THE AUTHOR USE THAT WORD WHAT WAS THEIR INTENT DOES IT BACK UP THEIR ARGUMENT THEY CAN START TO DOSUFF THAT ONCE AGAIN WE NEVER HAD THE ECAPABILITY TO GIVE EVERYONE A TUTOR EVERYONE A RIDING COACH TO ACTUALLY DIG IN TO READING AT THIS LEVEL AND YOU CAN GO ON THE OTHER SIDE OF IT  WE HAVE A WHOLE WORK FLOWS AND HELPSOME RIHT HELPSOME BE A RIHTE A A RIDING COACH DRAW AN OUTLINE BUT ONCE A STUDENT ACTUALLY A CRINSTRUCTS A DRAFT AND THIS IS WHERE THEY ARE EARE CONSTRUCTING A DRAFT THEY CAN ASK FOR FEED BACK ONCE AGAIN AS YOU WOULD EXPECT FROM A GOOD RIDING COACH IN THIS CASE THE STUDENT WILL SAY EH AS IT DOES MY EVIDENCE SUPPORT MY CLAIM AND THEN THE A AI NOT ONLY IS I ABLE TO GIVE FEED BACK BUT ITS ABLE TO HIGHLIGHT CERTAIN PARTS OF THE PASSEN SAYS YOUKN ON THIS PASSAGE THIS DOESN'T QUITE SUPPORT YOUR CLAIM BUT ONCE AGAIN SOCRATICALLY SAYS CAN YOU TELL US WHY SO'S PULLING THE STUDENT IT'S MAKING THEM A BETTER RID OR GIVING THEM FAR MORE FEED BACK THAN THEY'VE EVER BEEN ABLE TO TO ACTUALLY GET BEFOR NE WE THINK THERE'S GOING TO DRAMATICALLY ACCELERATE WRITING NOT HURT IT NOW EVERYTHING I HAVE TALKED ABOUT SO FAR IS FOR THE STUDENT BUT WE THINK THIS COULD BE EQUALLY AS POWERFUL FOR THE TEACHER TO DRIVE MORE PERSONALIZED EDUCATION AND FRANKLY SAFE TIME AND ENERGY FOR THEMSELVES AND FOR THEIR STUDENTS SO THIS IS AN AMERICAN HISTORY EXERCISE ON CON ACATAMIZA QUESTION ABOUT THE SPANISH AMERICA ARZA DE A YASPETCHMANKAN WAR AND A AT FIRST ITS IN STUDENT MODE AND IF YOU SAY TELL ME THE ANSWER IT'S NOT GIG TTILL THE ANSWER IS GOING TO GO INTO TUDORING MODE BUT A LITTLE TOGGLE WHICH TEACHERS HAVE ACCES TO THEY CAN TURN STUDENT MODE OFF AND THEN IT GOES INTO TEACHER MODE AND WHAT THIS DOES IS IT TURN NCE INTO YOU COULD VIEW IT AS A TEACHER'S GUIDE ON STEROIS NOT ONLY CAN IT EXPLAIN THE ANSWER IT CAN EXPLAIN HOW YOU MIGHT WANT TO TEACH IT IT CAN HELP PREPARE THE STUTE THE TEACHER FOR THAT MATERIAL IT CAN HELP THEM CREATE LESSON PLANS AS YOU COULD SEE A DOING RIGHT THERE IT'LL EVENTUALLY HELP THEM CREAT PROGRESS REPORTS TO HELP THEM  EVENTULLY GRADE SO ONCE AGAIN TEACH URS MEN HALF THEIR TIME WITH THIS TYPE OF ACTIVITY LESSEN PLANTING ALL THAT ENERGY CAN GO BACK TO THEM OR GO BACK TO HUMAN INTERACTIONS WITH THEIR ACTUAL STUDENTS THOUGH YOU KNOW ONE POINT I WANT TO MAKE THESE LARGE LANGUAGE MODELS ARE SO POWERFUL THERE'S A T EMPTATION TO SAY LIKE WELL ALL THESE PEOPLE AREJUS GOIN TO SLAP THEM ON TER THEIR WEBSIDES AND IT KIND OF TURNS THE APPLICATIONS THEMSELVES INTO INTO COMMODITIES AND O WHAT I GOT TO TELL YOUS I CANT A THOUGHT THAT THAT'S ONE OF THE REASONS WHY DIDN'T SLEEP FOR TWO WEEKS WHEN WHEN I FIRST HAD ACCESS TO A DIPUTY FOR BACK IN AUGUST BUT WE QUICKLY REALIZE THAT TO ACTUALLY MAKE IT MAGICAL I THINK WHAT YOU SAW WITH CONMEGO LITTLE IT IT DIDN'T INTERACT WITH YOU THE WAY THAT YOU SEE CHATCHE BE TEENERACTING YOU DO S A LITTLE BIT MORE MAGICAL IT WAS MORE SOCRATIC IT WAS CLEARLY MUCH BETTER AT MATH THAN WHAT MOST PEOPLE ARE USED TO TO THINKING AND THE REASON IS THERE WAS A LOT OF WORK BEHIND THE SCENES TO MAKE THAT HAPPEN AND I COULD GO THROUGH THE WHOLE LIST OF EVERYTHING WE'VE BEEN WORKING ON MANY MANY PEOPLE FOR OVER SIX SEVEN MONTHS TO MAKE IT FEEL MAGICAL BUT PERHAPS THE MOST INTELLECTUALLY INTERESTING ONE IS WE REALIZED THAT THIS WAS AN IDEA FROM AN OPENING EYE RESEARCHER THAT WE COULD DRAMATICALLY IMPROVE ITS ABILITY IN MATH AND ITS ABILITY IN TUTORING IF WE ALLOWED THE A I TO THINK BEFORE IT SPEAKS SO IF YOU ARE TUTORING SOMEONE AND YOU MEDIATELY JUST ARRU OUR TALKING BEFORE YOU ASSESS THEIR MATH YOU MIG N I GET IT RIGHT BUT IF YOU CONSTRUCT THOUGHTS FOR YOURSELF AND WHAT YOU SEE ON THE RIGHT THERE IS AN ACTUAL A I THOUGHT SOMETHING THAT IT GENERATES FOR ITSELF BUT IT DOES NOT SHARE WITH THE STUDENT THEN IT'S ACCURACY WENT UP DRAMATICALLY AND IT'S ABILITY TO BE A WORLDCLASS TUTOR WENT UP DRAMATICALL AND YOU CAN SEE IT'S TALKING TO ITSELF HERE IT SAYS THE STUDENT GOT A DIFFERENT ANSWER THAN I DID BUT DO NOT TELL THEM THEY ADE A MISTAKE INSTEAD ASK THEM TO PLAIN EXPLAIN HOW THEY GOT TO THAT THAT STEP SO I'LL JUST FINISH OFF HOPEFULLY YOU KNOW WHAT I'VE JUST SHOWN YOU JUST HALF OF WHAT WE ARE WORKING ON AND WE THINK IS THIS IS JUST THE VERY TIP OF THE ICEBERG OF WHERE THIS THIS CAN ACTUALLY GO AND I'M PRETTY CONVINCED I I WOULDN'T HAVE BEEN EVEN A YEAR AGO THAT WE TOGETHER HAVE A CHANCE OF ADDRESSING THE TO SIGMAL PROBLEM AND TURNING IT INTO A TOO SIGMA OPPORTUNITY DRAMATICALLY ACCELERATING A EDUCATION AS AS WE KNOW IT NOW JUST TO TAKE A STEP BACK AT A METAL OF LOVSO WE HEARD A LOT TO DAY THE DEBATES ON EITHER SIDE THERE'S FOLKS WHO A TIK A MORE PESSIMISTIC VIEW OF A I THEY SAY THIS TI SCARY THERE'S ALL THESE DISTOPIAN SENARIOS WE MAYBE BUT WANT O SLOW DOWN WE WE WANT O PAUSE ON THE OTHER SIDE THEREARE THE MORE OPTIMISTIC FOLKS SAY WELL WE'VE GONE THROUGH INFLECTION POINTS BEFORE WE'VE GONE THROUGH THE INDUSTRIAL REVOLUTION AND IT WAS SCARY BUT IT ALL KIND OF WORKED OUT MD WHAT I'D ARGUE RIGHT NOW IS I DON'T THINK THIS IS LIKE A FLIP OF A COIN OR THIS IS SOMETHING WHERE WE'LL JUST HAVE TO LIKE WAIT AND SEE WHICH WAY IT TURNS OUT I THINK EVERY ONE HERE AND BEYOND WE ARE ACTIVE PARTICIPANTS IN THIS DECISION I'M PRETTY CONVINCED THAT THE FIRST LINE OF REASONING IS ACTUALLY ALMOST A SELF FULFILLING PROPHECY THAT IF WE ACT WITH FEAR AND IF WE SAY HEY WE JUST GOT A STOP DOING THIS STUFF WHAT'S REALLY GOING TO HAPPEN AS THE RULE FOLLOWERS MIGHT PAUSE MIGHT FLOW DOWN BUT THE RULE BREAKERS AS ALEXANDER MENTIONED THE TETALITARIAN GOVERNMENTS THE CRIMINAL ORGANIZATIONS THEY'RE ONLY GOING TO ACCELERATE AND THAT LEADS TO WHAT I AM PRETTY CONVINCED IS THE DISTOP IAN STATE WHICH IS THE GOOD ACTORS HAVE WORSE A EYS THAN THE BAD ACTORS BUT I'LL ALSO YOU KNOW TALK TO THE OPTIMIST A LITTLE BIT I DON'T THINK THAT MEANS THAT OH YE THEN WE SHOULD JUST RELAX AND JUST HOPE FOR THE BEST THAT MIGHT NOT HAPPEN EITHER I THINK ALL OF US TOGETHER HAVE TO FIGHT LIKE HELL TO MAKE SURE THAT WE PUT THE GUARD RAILS WE PUT IN WHEN WHEN THE PROBLEMS ARISE REASONABLE REGULATIONS BUT WE FIGHT LIKE HELL FOR THE POSITIVE USECASES BECAUSE VERY CLOSE TO MY HEART AND I'VE SEE THERE'S MANY POTENTIAL POSIVE USECASES BUT PERHAPS THE MOST POWERFUL USECASE AND A PERHAPS THE MOST POETIC USECASE IS IF A I ARTIFICIAL INTELLIGENCE CAN BE USED TO ENHANCE H I HUMAN INTELLIGENCE HUMAN POTENTIAL AND HUMAN PURPOSE THANK YOU\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted transcribed audio:\", combined_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual transcribed audio: SO ANYONE WHO'S BEEN PAYING ATTENTION FOR THE LAST FEW MONTHS HAS BEEN SEEING HEADLINES LIKE THIS, ESPECIALLY IN EDUCATION. THE THESIS HAS BEEN: STUDENTS ARE GOING TO BE USING CHATGPT AND OTHER FORMS OF AI TO CHEAT, DO THEIR ASSIGNMENTS. THEY’RE NOT GOING TO LEARN. AND IT’S GOING TO COMPLETELY UNDERMINE EDUCATION AS WE KNOW IT. NOW, WHAT I'M GOING TO ARGUE TODAY IS NOT ONLY ARE THERE WAYS TO MITIGATE ALL OF THAT, IF WE PUT THE RIGHT GUARDRAILS, WE DO THE RIGHT THINGS, WE CAN MITIGATE IT. BUT I THINK WE'RE AT THE CUSP OF USING AI FOR PROBABLY THE BIGGEST POSITIVE TRANSFORMATION THAT EDUCATION HAS EVER SEEN. AND THE WAY WE'RE GOING TO DO THAT IS BY GIVING EVERY STUDENT ON THE PLANET AN ARTIFICIALLY INTELLIGENT BUT AMAZING PERSONAL TUTOR. AND WE'RE GOING TO GIVE EVERY TEACHER ON THE PLANET AN AMAZING, ARTIFICIALLY INTELLIGENT TEACHING ASSISTANT. AND JUST TO APPRECIATE HOW BIG OF A DEAL IT WOULD BE TO GIVE EVERYONE A PERSONAL TUTOR, I SHOW YOU THIS CLIP FROM BENJAMIN BLOOM’S 1984 2 SIGMA STUDY, OR HE CALLED IT THE “2 SIGMA PROBLEM.” THE 2 SIGMA COMES FROM TWO STANDARD DEVIATION, SIGMA, THE SYMBOL FOR STANDARD DEVIATION. AND HE HAD GOOD DATA THAT SHOWED THAT LOOK, A NORMAL DISTRIBUTION, THAT'S THE ONE THAT YOU SEE IN THE TRADITIONAL BELL CURVE RIGHT IN THE MIDDLE, THAT'S HOW THE WORLD KIND OF SORTS ITSELF OUT, THAT IF YOU WERE TO GIVE PERSONAL 1-TO-1 TO TUTORING FOR STUDENTS, THEN YOU COULD ACTUALLY GET A DISTRIBUTION THAT LOOKS LIKE THAT RIGHT. IT SAYS TUTORIAL 1-TO-1 WITH THE ASTERISKS, LIKE, THAT RIGHT DISTRIBUTION, A TWO STANDARD-DEVIATION IMPROVEMENT. JUST TO PUT THAT IN PLAIN LANGUAGE, THAT COULD TAKE YOUR AVERAGE STUDENT AND TURN THEM INTO AN EXCEPTIONAL STUDENT. IT CAN TAKE YOUR BELOW-AVERAGE STUDENT AND TURN THEM INTO AN ABOVE-AVERAGE STUDENT. NOW THE REASON WHY HE FRAMED IT AS A PROBLEM, WAS HE SAID, WELL, THIS IS ALL GOOD, BUT HOW DO YOU ACTUALLY SCALE GROUP INSTRUCTION THIS WAY? HOW DO YOU ACTUALLY GIVE IT TO EVERYONE IN AN ECONOMIC WAY? WHAT I'M ABOUT TO SHOW YOU IS I THINK THE FIRST MOVES TOWARDS DOING THAT. OBVIOUSLY, WE'VE BEEN TRYING TO APPROXIMATE IT IN SOME WAY AT KHAN ACADEMY FOR OVER A DECADE NOW, BUT I THINK WE'RE AT THE CUSP OF ACCELERATING IT DRAMATICALLY. I'M GOING TO SHOW YOU THE EARLY STAGES OF WHAT OUR AI, WHICH WE CALL KHANMIGO, WHAT IT CAN NOW DO AND MAYBE A LITTLE BIT OF WHERE IT IS ACTUALLY GOING. SO THIS RIGHT OVER HERE IS A TRADITIONAL EXERCISE THAT YOU OR MANY OF YOUR CHILDREN MIGHT HAVE SEEN ON KHAN ACADEMY. BUT WHAT'S NEW IS THAT LITTLE BOT THING AT THE RIGHT. AND WE'LL START BY SEEING ONE OF THE VERY IMPORTANT SAFEGUARDS, WHICH IS THE CONVERSATION IS RECORDED AND VIEWABLE BY YOUR TEACHER. IT’S MODERATED ACTUALLY BY A SECOND AI. AND ALSO IT DOES NOT TELL YOU THE ANSWER. IT IS NOT A CHEATING TOOL. WHEN THE STUDENT SAYS, \"TELL ME THE ANSWER,\" IT SAYS, \"I'M YOUR TUTOR. WHAT DO YOU THINK IS THE NEXT STEP FOR SOLVING THE PROBLEM?\" NOW, IF THE STUDENT MAKES A MISTAKE, AND THIS WILL SURPRISE PEOPLE WHO THINK LARGE LANGUAGE MODELS ARE NOT GOOD AT MATHEMATICS, NOTICE, NOT ONLY DOES IT NOTICE THE MISTAKE, IT ASKS THE STUDENT TO EXPLAIN THEIR REASONING, BUT IT'S ACTUALLY DOING WHAT I WOULD SAY, NOT JUST EVEN AN AVERAGE TUTOR WOULD DO, BUT AN EXCELLENT TUTOR WOULD DO. IT’S ABLE TO DIVINE WHAT IS PROBABLY THE MISCONCEPTION IN THAT STUDENT’S MIND, THAT THEY PROBABLY DIDN’T USE THE DISTRIBUTIVE PROPERTY. REMEMBER, WE NEED TO DISTRIBUTE THE NEGATIVE TWO TO BOTH THE NINE AND THE 2M INSIDE OF THE PARENTHESES. THIS TO ME IS A VERY, VERY, VERY BIG DEAL. AND IT'S NOT JUST IN MATH. THIS IS A COMPUTER PROGRAMMING EXERCISE ON KHAN ACADEMY, WHERE THE STUDENT NEEDS TO MAKE THE CLOUDS PART. AND SO WE CAN SEE THE STUDENT STARTS DEFINING A VARIABLE, LEFT X MINUS MINUS. IT ONLY MADE THE LEFT CLOUD PART. BUT THEN THEY CAN ASK KHANMIGO, WHAT’S GOING ON? WHY IS ONLY THE LEFT CLOUD MOVING? AND IT UNDERSTANDS THE CODE. IT KNOWS ALL THE CONTEXT OF WHAT THE STUDENT IS DOING, AND IT UNDERSTANDS THAT THOSE ELLIPSES ARE THERE TO DRAW CLOUDS, WHICH I THINK IS KIND OF MIND-BLOWING. AND IT SAYS, \"TO MAKE THE RIGHT CLOUD MOVE AS WELL, TRY ADDING A LINE OF CODE INSIDE THE DRAW FUNCTION THAT INCREMENTS THE RIGHT X VARIABLE BY ONE PIXEL IN EACH FRAME.\" NOW, THIS ONE IS MAYBE EVEN MORE AMAZING BECAUSE WE HAVE A LOT OF MATH TEACHERS. WE'VE ALL BEEN TRYING TO TEACH THE WORLD TO CODE, BUT THERE AREN'T A LOT OF COMPUTING TEACHERS OUT THERE. AND WHAT YOU JUST SAW, EVEN WHEN I'M TUTORING MY KIDS, WHEN THEY'RE LEARNING TO CODE, I CAN'T HELP THEM THIS WELL, THIS FAST, THIS IS REALLY GOING TO BE A SUPER TUTOR. AND IT'S NOT JUST EXERCISES. IT UNDERSTANDS WHAT YOU'RE WATCHING. IT UNDERSTANDS THE CONTEXT OF YOUR VIDEO. IT CAN ANSWER THE AGE-OLD QUESTION, “WHY DO I NEED TO LEARN THIS?” AND IT ASKS SOCRATICALLY, \"WELL, WHAT DO YOU CARE ABOUT?\" AND LET'S SAY THE STUDENT SAYS, \"I WANT TO BE A PROFESSIONAL ATHLETE.\" AND IT SAYS, \"WELL, LEARNING ABOUT THE SIZE OF CELLS, WHICH IS WHAT THIS VIDEO IS, THAT COULD BE REALLY USEFUL FOR UNDERSTANDING NUTRITION AND HOW YOUR BODY WORKS, ETC.\" IT CAN ANSWER QUESTIONS, IT CAN QUIZ YOU, IT CAN CONNECT IT TO OTHER IDEAS, YOU CAN NOW ASK AS MANY QUESTIONS OF A VIDEO AS YOU COULD EVER DREAM OF. ANOTHER BIG SHORTAGE OUT THERE, I REMEMBER THE HIGH SCHOOL I WENT TO, THE STUDENT-TO-GUIDANCE COUNSELOR RATIO WAS ABOUT 200 OR 300 TO ONE. A LOT OF THE COUNTRY, IT'S WORSE THAN THAT. WE CAN USE KHANMIGO TO GIVE EVERY STUDENT A GUIDANCE COUNSELOR, ACADEMIC COACH, CAREER COACH, LIFE COACH, WHICH IS EXACTLY WHAT YOU SEE RIGHT OVER HERE. AND WE LAUNCHED THIS WITH THE GPT-4 LAUNCH. WE HAVE A FEW THOUSAND PEOPLE ON THIS. THIS ISN'T A FAKE DEMO, THIS IS REALLY IT IN ACTION. AND THEN THERE IS, YOU KNOW, THINGS THAT I THINK IT WOULD HAVE BEEN EVEN HARDER, IT WOULD HAVE BEEN A LITTLE SCIENCE FICTION TO DO WITH EVEN A TRADITIONAL TUTOR. WE RUN AN ONLINE HIGH SCHOOL WITH ARIZONA STATE UNIVERSITY CALLED KHAN WORLD SCHOOL, AND WE HAVE A STUDENT WHO ATTENDS THAT ONLINE SCHOOL, BASED IN INDIA. HER NAME'S SAANVI. AND SHE WAS DOING A REPORT ON \"THE GREAT GATSBY.\" AND WHEN SHE WAS READING \"THE GREAT GATSBY,\" JAY GATSBY KEEPS LOOKING AT THE GREEN LIGHT OFF INTO THE DISTANCE. AND SHE'S LIKE, \"WHY DOES HE DO THAT?\" SHE DID SOME WEB SEARCHES, AND PEOPLE HAVE OBVIOUSLY STUDIED THIS AND COMMENTED ABOUT THE SYMBOLISM OF THAT, BUT NONE OF IT WAS REALLY RESONATING WITH HER. AND THEN SHE REALIZED THAT SHE HAD KHANMIGO AND THAT SHE COULD TALK TO JAY GATSBY HIMSELF. AND SO, \"AH, SPLENDID CHOICE, OLD SPORT. I AM NOW JAY GATSBY, THE ENIGMATIC MILLIONAIRE FROM F. SCOTT FITZGERALD’S CLASSIC.” AND SO, \"WHY DO YOU KEEP STARING AT THE GREEN LIGHT?\" \"AH, THE GREEN LIGHT, OLD SPORT. IT'S A SYMBOL OF MY DREAMS AND DESIRES, YOU SEE. IT'S SITUATED AT THE END OF DAISY BUCHANAN'S DOCK ACROSS THE BAY FROM MY MANSION. I GAZE AT IT LONGINGLY AS IT REPRESENTS MY YEARNING FOR THE PAST AND MY HOPE TO REUNITE WITH DAISY, THE LOVE OF MY LIFE.\" AND WHAT WAS COOL IS, SAANVI THEN SAID, “I HAD THIS LONG CONVERSATION,” SHE CALLED HIM “MR. GATSBY,” AND AT THE END SHE ACTUALLY APOLOGIZED FOR TAKING HIS TIME, WHICH I THOUGHT WAS VERY POLITE OF HER. BUT YOU CAN IMAGINE THIS UNLOCKS LEARNING LITERATURE, LEARNING ... YOU COULD TALK TO HISTORICAL FIGURES. WE'RE EVEN PROBABLY GOING TO ADD AN ACTIVITY YOU CAN TALK TO LIKE, THE MISSISSIPPI RIVER. IT BRINGS THINGS TO LIFE IN WAYS THAT REALLY WERE SCIENCE FICTION EVEN SIX MONTHS OR A YEAR AGO. STUDENTS CAN GET INTO DEBATES WITH THE AI. AND WE’VE GOT THIS HERE IS THE STUDENT DEBATING WHETHER WE SHOULD CANCEL STUDENT DEBT. THE STUDENT IS AGAINST CANCELING STUDENT DEBT, AND WE'VE GOTTEN VERY CLEAR FEEDBACK. WE STARTED RUNNING IT AT KHAN WORLD SCHOOL IN OUR LAB SCHOOL THAT WE HAVE, KHAN LAB SCHOOL. THE STUDENTS, THE HIGH SCHOOL STUDENTS ESPECIALLY, THEY'RE SAYING \"THIS IS AMAZING TO BE ABLE TO FINE-TUNE MY ARGUMENTS WITHOUT FEARING JUDGMENT. IT MAKES ME THAT MUCH MORE CONFIDENT TO GO INTO THE CLASSROOM AND REALLY PARTICIPATE.\" AND WE ALL KNOW THAT SOCRATIC DIALOGUE DEBATE IS A GREAT WAY TO LEARN, BUT FRANKLY, IT'S NOT OUT THERE FOR MOST STUDENTS. BUT NOW IT CAN BE ACCESSIBLE TO HOPEFULLY EVERYONE. A LOT OF THE NARRATIVE, WE SAW THAT IN THE HEADLINES, HAS BEEN, \"IT'S GOING TO DO THE WRITING FOR KIDS. KIDS ARE NOT GOING TO LEARN TO WRITE.\" BUT WE ARE SHOWING THAT THERE'S WAYS THAT THE AI DOESN'T WRITE FOR YOU, IT WRITES WITH YOU. SO THIS IS A LITTLE THING, AND MY EIGHT YEAR OLD IS ADDICTED TO THIS, AND HE'S NOT A KID THAT REALLY LIKED WRITING BEFORE, BUT YOU CAN SAY, “I WANT TO WRITE A HORROR STORY,” AND IT SAYS, \"OOH, A HORROR STORY, HOW SPINE-TINGLING AND THRILLING. LET'S DIVE INTO THE WORLD OF EERIE SHADOWS AND CHILLING MYSTERIES.\" AND THIS IS AN ACTIVITY WHERE THE STUDENT WILL WRITE TWO SENTENCES, AND THEN THE AI WILL WRITE TWO SENTENCES. AND SO THEY COLLABORATE TOGETHER ON A STORY. THE STUDENT WRITES, \"BEATRICE WAS A MISUNDERSTOOD GHOST. SHE WANTED TO MAKE FRIENDS BUT KEPT SCARING THEM BY ACCIDENT.\" AND THE AI SAYS, \"POOR BEATRICE, A LONELY SPIRIT YEARNING FOR COMPANIONSHIP. ONE DAY SHE STUMBLED UPON AN OLD ABANDONED MANSION,\" ETC. I ENCOURAGE YOU ALL TO HOPEFULLY ONE DAY TRY THIS. THIS IS SURPRISINGLY FUN. NOW TO EVEN MORE DIRECTLY HIT THIS USE CASE. AND WHAT I'M ABOUT TO SHOW YOU, EVERYTHING I SHOWED YOU SO FAR IS ACTUALLY ALREADY PART OF KHANMIGO, AND WHAT I’M ABOUT TO SHOW YOU, WE HAVEN'T SHOWN TO ANYONE YET, THIS IS A PROTOTYPE. WE HOPE TO BE ABLE TO LAUNCH IT IN THE NEXT FEW MONTHS, BUT THIS IS TO DIRECTLY USE AI, USE GENERATIVE AI, TO NOT UNDERMINE ENGLISH AND LANGUAGE ARTS BUT TO ACTUALLY ENHANCE IT IN WAYS THAT WE COULDN'T HAVE EVEN CONCEIVED OF EVEN A YEAR AGO. THIS IS READING COMPREHENSION. THE STUDENTS READING STEVE JOBS'S FAMOUS SPEECH AT STANFORD. AND THEN AS THEY GET TO CERTAIN POINTS, THEY CAN CLICK ON THAT LITTLE QUESTION. AND THE AI WILL THEN SOCRATICALLY, ALMOST LIKE AN ORAL EXAM, ASK THE STUDENT ABOUT THINGS. AND THE AI CAN HIGHLIGHT PARTS OF THE PASSAGE. WHY DID THE AUTHOR USE THAT WORD? WHAT WAS THEIR INTENT? DOES IT BACK UP THEIR ARGUMENT? THEY CAN START TO DO STUFF THAT ONCE AGAIN, WE NEVER HAD THE CAPABILITY TO GIVE EVERYONE A TUTOR, EVERYONE A WRITING COACH TO ACTUALLY DIG IN TO READING AT THIS LEVEL. AND YOU COULD GO ON THE OTHER SIDE OF IT. AND WE HAVE WHOLE WORK FLOWS THAT HELPS THEM WRITE, HELPS THEM BE A WRITING COACH, DRAW AN OUTLINE. BUT ONCE A STUDENT ACTUALLY CONSTRUCTS A DRAFT, AND THIS IS WHERE THEY'RE CONSTRUCTING A DRAFT, THEY CAN ASK FOR FEEDBACK ONCE AGAIN, AS YOU WOULD EXPECT FROM A GOOD WRITING COACH. IN THIS CASE, THE STUDENT WILL SAY, LET'S SAY, \"DOES MY EVIDENCE SUPPORT MY CLAIM?\" AND THEN THE AI, NOT ONLY IS ABLE TO GIVE FEEDBACK, BUT IT'S ABLE TO HIGHLIGHT CERTAIN PARTS OF THE PASSAGE AND SAYS, \"ON THIS PASSAGE, THIS DOESN'T QUITE SUPPORT YOUR CLAIM,\" BUT ONCE AGAIN, SOCRATICALLY SAYS, \"CAN YOU TELL US WHY?\" SO IT'S PULLING THE STUDENT, MAKING THEM A BETTER WRITER, GIVING THEM FAR MORE FEEDBACK THAN THEY'VE EVER BEEN ABLE TO ACTUALLY GET BEFORE. AND WE THINK THIS IS GOING TO DRAMATICALLY ACCELERATE WRITING, NOT HURT IT. NOW, EVERYTHING I'VE TALKED ABOUT SO FAR IS FOR THE STUDENT. BUT WE THINK THIS COULD BE EQUALLY AS POWERFUL FOR THE TEACHER TO DRIVE MORE PERSONALIZED EDUCATION AND FRANKLY SAVE TIME AND ENERGY FOR THEMSELVES AND FOR THEIR STUDENTS. SO THIS IS AN AMERICAN HISTORY EXERCISE ON KHAN ACADEMY. IT'S A QUESTION ABOUT THE SPANISH-AMERICAN WAR. AND AT FIRST IT'S IN STUDENT MODE. AND IF YOU SAY, “TELL ME THE ANSWER,” IT’S NOT GOING TO TELL THE ANSWER. IT'S GOING TO GO INTO TUTORING MODE. BUT THAT LITTLE TOGGLE WHICH TEACHERS HAVE ACCESS TO, THEY CAN TURN STUDENT MODE OFF AND THEN IT GOES INTO TEACHER MODE. AND WHAT THIS DOES IS IT TURNS INTO -- YOU COULD VIEW IT AS A TEACHER'S GUIDE ON STEROIDS. NOT ONLY CAN IT EXPLAIN THE ANSWER, IT CAN EXPLAIN HOW YOU MIGHT WANT TO TEACH IT. IT CAN HELP PREPARE THE TEACHER FOR THAT MATERIAL. IT CAN HELP THEM CREATE LESSON PLANS, AS YOU COULD SEE DOING RIGHT THERE. IT'LL EVENTUALLY HELP THEM CREATE PROGRESS REPORTS AND HELP THEM, EVENTUALLY, GRADE. SO ONCE AGAIN, TEACHERS SPEND ABOUT HALF THEIR TIME WITH THIS TYPE OF ACTIVITY, LESSON PLANNING. ALL OF THAT ENERGY CAN GO BACK TO THEM OR GO BACK TO HUMAN INTERACTIONS WITH THEIR ACTUAL STUDENTS. SO, YOU KNOW, ONE POINT I WANT TO MAKE. THESE LARGE LANGUAGE MODELS ARE SO POWERFUL, THERE'S A TEMPTATION TO SAY LIKE, WELL, ALL THESE PEOPLE ARE JUST GOING TO SLAP THEM ONTO THEIR WEBSITES, AND IT KIND OF TURNS THE APPLICATIONS THEMSELVES INTO COMMODITIES. AND WHAT I'VE GOT TO TELL YOU IS THAT’S ONE OF THE REASONS WHY I DIDN’T SLEEP FOR TWO WEEKS WHEN I FIRST HAD ACCESS TO GPT-4 BACK IN AUGUST. BUT WE QUICKLY REALIZED THAT TO ACTUALLY MAKE IT MAGICAL, I THINK WHAT YOU SAW WITH KHANMIGO A LITTLE BIT, IT DIDN'T INTERACT WITH YOU THE WAY THAT YOU SEE CHATGPT INTERACTING. IT WAS A LITTLE BIT MORE MAGICAL, IT WAS MORE SOCRATIC, IT WAS CLEARLY MUCH BETTER AT MATH THAN WHAT MOST PEOPLE ARE USED TO THINKING. AND THE REASON IS, THERE WAS A LOT OF WORK BEHIND THE SCENES TO MAKE THAT HAPPEN. AND I COULD GO THROUGH THE WHOLE LIST OF EVERYTHING WE'VE BEEN WORKING ON, MANY, MANY PEOPLE FOR OVER SIX, SEVEN MONTHS TO MAKE IT FEEL MAGICAL. BUT PERHAPS THE MOST INTELLECTUALLY INTERESTING ONE IS WE REALIZED, AND THIS WAS AN IDEA FROM AN OPENAI RESEARCHER, THAT WE COULD DRAMATICALLY IMPROVE ITS ABILITY IN MATH AND ITS ABILITY IN TUTORING IF WE ALLOW THE AI TO THINK BEFORE IT SPEAKS. SO IF YOU'RE TUTORING SOMEONE AND YOU IMMEDIATELY JUST START TALKING BEFORE YOU ASSESS THEIR MATH, YOU MIGHT NOT GET IT RIGHT. BUT IF YOU CONSTRUCT THOUGHTS FOR YOURSELF, AND WHAT YOU SEE ON THE RIGHT THERE IS AN ACTUAL AI THOUGHT, SOMETHING THAT IT GENERATES FOR ITSELF BUT IT DOES NOT SHARE WITH THE STUDENT. THEN ITS ACCURACY WENT UP DRAMATICALLY, AND ITS ABILITY TO BE A WORLD-CLASS TUTOR WENT UP DRAMATICALLY. AND YOU CAN SEE IT'S TALKING TO ITSELF HERE. IT SAYS, \"THE STUDENT GOT A DIFFERENT ANSWER THAN I DID, BUT DO NOT TELL THEM THEY MADE A MISTAKE. INSTEAD, ASK THEM TO EXPLAIN HOW THEY GOT TO THAT STEP.\" SO I'LL JUST FINISH OFF, HOPEFULLY, YOU KNOW, WHAT I’VE JUST SHOWN YOU IS JUST HALF OF WHAT WE ARE WORKING ON, AND WE THINK THIS IS JUST THE VERY TIP OF THE ICEBERG OF WHERE THIS CAN ACTUALLY GO. AND I'M PRETTY CONVINCED, WHICH I WOULDN'T HAVE BEEN EVEN A YEAR AGO, THAT WE TOGETHER HAVE A CHANCE OF ADDRESSING THE 2 SIGMA PROBLEM AND TURNING IT INTO A 2 SIGMA OPPORTUNITY, DRAMATICALLY ACCELERATING EDUCATION AS WE KNOW IT. NOW, JUST TO TAKE A STEP BACK AT A META LEVEL, OBVIOUSLY WE HEARD A LOT TODAY, THE DEBATES ON EITHER SIDE. THERE'S FOLKS WHO TAKE A MORE PESSIMISTIC VIEW OF AI, THEY SAY THIS IS SCARY, THERE'S ALL THESE DYSTOPIAN SCENARIOS, WE MAYBE WANT TO SLOW DOWN, WE WANT TO PAUSE. ON THE OTHER SIDE, THERE ARE THE MORE OPTIMISTIC FOLKS THAT SAY, WELL, WE'VE GONE THROUGH INFLECTION POINTS BEFORE, WE'VE GONE THROUGH THE INDUSTRIAL REVOLUTION. IT WAS SCARY, BUT IT ALL KIND OF WORKED OUT. AND WHAT I'D ARGUE RIGHT NOW IS I DON'T THINK THIS IS LIKE A FLIP OF A COIN OR THIS IS SOMETHING WHERE WE'LL JUST HAVE TO, LIKE, WAIT AND SEE WHICH WAY IT TURNS OUT. I THINK EVERYONE HERE AND BEYOND, WE ARE ACTIVE PARTICIPANTS IN THIS DECISION. I'M PRETTY CONVINCED THAT THE FIRST LINE OF REASONING IS ACTUALLY ALMOST A SELF-FULFILLING PROPHECY, THAT IF WE ACT WITH FEAR AND IF WE SAY, \"HEY, WE'VE JUST GOT TO STOP DOING THIS STUFF,\" WHAT'S REALLY GOING TO HAPPEN IS THE RULE FOLLOWERS MIGHT PAUSE, MIGHT SLOW DOWN, BUT THE RULE BREAKERS, AS ALEXANDR [WANG] MENTIONED, THE TOTALITARIAN GOVERNMENTS, THE CRIMINAL ORGANIZATIONS, THEY'RE ONLY GOING TO ACCELERATE. AND THAT LEADS TO WHAT I AM PRETTY CONVINCED IS THE DYSTOPIAN STATE, WHICH IS THE GOOD ACTORS HAVE WORSE AIS THAN THE BAD ACTORS. BUT I'LL ALSO, YOU KNOW, TALK TO THE OPTIMISTS A LITTLE BIT. I DON'T THINK THAT MEANS THAT, OH, YEAH, THEN WE SHOULD JUST RELAX AND JUST HOPE FOR THE BEST. THAT MIGHT NOT HAPPEN EITHER. I THINK ALL OF US TOGETHER HAVE TO FIGHT LIKE HELL TO MAKE SURE THAT WE PUT THE GUARDRAILS, WE PUT IN -- WHEN THE PROBLEMS ARISE -- REASONABLE REGULATIONS. BUT WE FIGHT LIKE HELL FOR THE POSITIVE USE CASES. BECAUSE VERY CLOSE TO MY HEART, AND OBVIOUSLY THERE'S MANY POTENTIAL POSITIVE USE CASES, BUT PERHAPS THE MOST POWERFUL USE CASE AND PERHAPS THE MOST POETIC USE CASE IS IF AI, ARTIFICIAL INTELLIGENCE, CAN BE USED TO ENHANCE HI, HUMAN INTELLIGENCE, HUMAN POTENTIAL AND HUMAN PURPOSE. THANK YOU\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual transcribed audio:\", actual_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER): 0.30\n",
      "Character Error Rate (CER): 0.11\n"
     ]
    }
   ],
   "source": [
    "ground_truth = combined_transcription\n",
    "hypothesis = actual_transcript\n",
    "\n",
    "wer = editdistance.eval(ground_truth.split(), hypothesis.split()) / len(ground_truth.split())\n",
    "\n",
    "# Calculate CER\n",
    "cer = editdistance.eval(ground_truth, hypothesis) / len(ground_truth)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Word Error Rate (WER): {wer:.2f}\")\n",
    "print(f\"Character Error Rate (CER): {cer:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TED_Talk_Transcript.txt\", \"r\") as f:\n",
    "    actual_transcript = f.read().strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TextRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the way we're going to do that is by giving every student on the planet an artificially intelligent but amazing personal tutor.\n",
      "And he had good data that showed that look, a normal distribution, that's the one that you see in the traditional bell curve right in the middle, that's how the world kind of sorts itself out, that if you were to give personal 1-to-1 to tutoring for students, then you could actually get a distribution that looks like that right.\n",
      "It knows all the context of what the student is doing, and it understands that those ellipses are there to draw clouds, which I think is kind of mind-blowing.\n",
      "It can answer the age-old question, “Why do I need to learn this?” And it asks Socratically, \"Well, what do you care about?\" And let's say the student says, \"I want to be a professional athlete.\" And it says, \"Well, learning about the size of cells, which is what this video is, that could be really useful for understanding nutrition and how your body works, etc.\" It can answer questions, it can quiz you, it can connect it to other ideas, you can now ask as many questions of a video as you could ever dream of.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(actual_transcript, ratio=0.03)\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Luhn's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm pretty convinced that the first line of reasoning is actually almost a self-fulfilling prophecy, that if we act with fear and if we say, \"Hey, we've just got to stop doing this stuff,\" what's really going to happen is the rule followers might pause, might slow down, but the rule breakers, as Alexandr [Wang] mentioned, the totalitarian governments, the criminal organizations, they're only going to accelerate. Now, if the student makes a mistake, and this will surprise people who think large language models are not good at mathematics, notice, not only does it notice the mistake, it asks the student to explain their reasoning, but it's actually doing what I would say, not just even an average tutor would do, but an excellent tutor would do. So this is a little thing, and my eight year old is addicted to this, and he's not a kid that really liked writing before, but you can say, “I want to write a horror story,” and it says, \"Ooh, a horror story, how spine-tingling and thrilling. And he had good data that showed that look, a normal distribution, that's the one that you see in the traditional bell curve right in the middle, that's how the world kind of sorts itself out, that if you were to give personal 1-to-1 to tutoring for students, then you could actually get a distribution that looks like that right.\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text into sentences and words\n",
    "sentences = sent_tokenize(actual_transcript)\n",
    "words = word_tokenize(actual_transcript)\n",
    "\n",
    "# calculate the frequency of each word\n",
    "freq = defaultdict(int)\n",
    "for word in words:\n",
    "    freq[word.lower()] += 1\n",
    "\n",
    "# rank the sentences based on the frequency of important words\n",
    "scores = defaultdict(int)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in freq:\n",
    "            scores[i] += freq[word]\n",
    "\n",
    "# select the top 3 sentences as the summary\n",
    "summary = ' '.join([sentences[idx] for idx in sorted(scores, key=scores.get, reverse=True)[:4]])\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.stemmers import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And it says, \"To make the right cloud move as well, try adding a line of code inside the draw function that increments the right X variable by one pixel in each frame.\"\n",
      "They can start to do stuff that once again, we never had the capability to give everyone a tutor, everyone a writing coach to actually dig in to reading at this level.\n",
      "But we think this could be equally as powerful for the teacher to drive more personalized education and frankly save time and energy for themselves and for their students.\n",
      "These large language models are so powerful, there's a temptation to say like, well, all these people are just going to slap them onto their websites, and it kind of turns the applications themselves into commodities.\n"
     ]
    }
   ],
   "source": [
    "# Create a parser object to parse the text\n",
    "parser = PlaintextParser.from_file('TED_Talk_Transcript.txt', Tokenizer(\"english\"))\n",
    "stemmer = Stemmer(\"english\")\n",
    "\n",
    "# Create a LSA summarizer object\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(\"english\")\n",
    "\n",
    "# Print the summary\n",
    "for sentence in summarizer(parser.document, 4):\n",
    "        print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students can get into debates with the AI.\n",
      "But we are showing that there's ways that the AI doesn't write for you, it writes with you.\n",
      "And the reason is, there was a lot of work behind the scenes to make that happen.\n",
      "Now, just to take a step back at a meta level, obviously we heard a lot today, the debates on either side.\n"
     ]
    }
   ],
   "source": [
    "# Create a parser object to parse the text\n",
    "parser = PlaintextParser.from_file('TED_Talk_Transcript.txt', Tokenizer(\"english\"))\n",
    "stemmer = Stemmer(\"english\")\n",
    "\n",
    "# Create a LSA summarizer object\n",
    "summarizer = LexRankSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(\"english\")\n",
    "\n",
    "# Print the summary\n",
    "for sentence in summarizer(parser.document, 4):\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, what I'm going to argue today is not only are there ways to mitigate all of that, if we put the right guardrails, we do the right things, we can mitigate it.And just to appreciate how big of a deal it would be to give everyone a personal tutor, I show you this clip from Benjamin Bloom’s 1984 2 sigma study, or he called it the “2 sigma problem.” The 2 sigma comes from two standard deviation, sigma, the symbol for standard deviation.And he had good data that showed that look, a normal distribution, that's the one that you see in the traditional bell curve right in the middle, that's how the world kind of sorts itself out, that if you were to give personal 1-to-1 to tutoring for students, then you could actually get a distribution that looks like that right.I'm going to show you the early stages of what our AI, which we call Khanmigo, what it can now do and maybe a little bit of where it is actually going.Now, if the student makes a mistake, and this will surprise people who think large language models are not good at mathematics, notice, not only does it notice the mistake, it asks the student to explain their reasoning, but it's actually doing what I would say, not just even an average tutor would do, but an excellent tutor would do.It knows all the context of what the student is doing, and it understands that those ellipses are there to draw clouds, which I think is kind of mind-blowing.So this is a little thing, and my eight year old is addicted to this, and he's not a kid that really liked writing before, but you can say, “I want to write a horror story,” and it says, \"Ooh, a horror story, how spine-tingling and thrilling.And what I'm about to show you, everything I showed you so far is actually already part of Khanmigo, and what I’m about to show you, we haven't shown to anyone yet, this is a prototype.We hope to be able to launch it in the next few months, but this is to directly use AI, use generative AI, to not undermine English and language arts but to actually enhance it in ways that we couldn't have even conceived of even a year ago.And then the AI, not only is able to give feedback, but it's able to highlight certain parts of the passage and says, \"On this passage, this doesn't quite support your claim,\" but once again, Socratically says, \"Can you tell us why?\"But perhaps the most intellectually interesting one is we realized, and this was an idea from an OpenAI researcher, that we could dramatically improve its ability in math and its ability in tutoring if we allow the AI to think before it speaks.So I'll just finish off, hopefully, you know, what I’ve just shown you is just half of what we are working on, and we think this is just the very tip of the iceberg of where this can actually go.And I'm pretty convinced, which I wouldn't have been even a year ago, that we together have a chance of addressing the 2 sigma problem and turning it into a 2 sigma opportunity, dramatically accelerating education as we know it.And what I'd argue right now is I don't think this is like a flip of a coin or this is something where we'll just have to, like, wait and see which way it turns out.I'm pretty convinced that the first line of reasoning is actually almost a self-fulfilling prophecy, that if we act with fear and if we say, \"Hey, we've just got to stop doing this stuff,\" what's really going to happen is the rule followers might pause, might slow down, but the rule breakers, as Alexandr [Wang] mentioned, the totalitarian governments, the criminal organizations, they're only going to accelerate.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "# For Strings\n",
    "parser = PlaintextParser.from_file(\"TED_Talk_Transcript.txt\",Tokenizer(\"english\"))\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = TextRankSummarizer()\n",
    "summary = summarizer(parser.document, 15)\n",
    "text_summary=\"\"\n",
    "\n",
    "for sentence in summary:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword & Keyphrases Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import app.text_samples as ts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Base BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = actual_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keywords: [('dystopian', 0.1881), ('accident', 0.1175), ('cheating', 0.1103), ('horror', 0.0984), ('fear', 0.0974)]\n"
     ]
    }
   ],
   "source": [
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keywords = model.extract_keywords(doc, stop_words='english', top_n=5)\n",
    "\n",
    "print(\"Top 5 Keywords:\", keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Max Sum Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases using Max Sum Similarity: [('pessimistic', 0.0602), ('war', 0.0652), ('tutorial', 0.0797), ('millionaire', 0.0874), ('teachers', 0.0887)]\n"
     ]
    }
   ],
   "source": [
    "# 2.3. Max Sum Similarity\n",
    "# To diversity the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n\n",
    "# words and extract the combination that are the least similar to each other by cosine similarity.\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keywords1 = model.extract_keywords(doc, stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(\"Top 5 Keyphrases using Max Sum Similarity:\", keywords1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keywords using Maximal Marginal Relevance: [('dystopian', 0.1881), ('teachers', 0.0887), ('millionaire', 0.0874), ('months', -0.0229), ('arizona', -0.0651)]\n"
     ]
    }
   ],
   "source": [
    "# 2.4. Maximal Marginal Relevance\n",
    "# To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity.\n",
    "# The results with high diversity:\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keywords2 = model.extract_keywords(doc, stop_words='english', use_mmr=True, diversity=0.7, top_n=5)\n",
    "print(\"Top 5 Keywords using Maximal Marginal Relevance:\", keywords2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keywords using Sentence Transformer: [('mathematics', 0.0202), ('university', 0.0234), ('pessimistic', 0.0602), ('tutoring', 0.0645), ('millionaire', 0.0874)]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\", device=\"cpu\")\n",
    "model = KeyBERT(model=sentence_model)\n",
    "keywords3 =  model.extract_keywords(doc, stop_words='english', use_maxsum=True, nr_candidates=30, top_n=5)\n",
    "print(\"Top 5 Keywords using Sentence Transformer:\", keywords3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Spacy Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keywords using Spacy Transformer: [('choice', 0.3606), ('just', 0.3679), ('opportunity', 0.3694), ('variable', 0.3702), ('fast', 0.3909)]\n"
     ]
    }
   ],
   "source": [
    "# Using spacy-transformer models:\n",
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "model = KeyBERT(model=nlp)\n",
    "keywords5 = model.extract_keywords(doc, stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(\"Top 5 Keywords using Spacy Transformer:\", keywords5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyphrases Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Base BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases: [('student debating cancel', 0.4364), ('scaring accident ai', 0.4301), ('student canceling student', 0.4219), ('industrial revolution scary', 0.4011), ('cheating tool student', 0.3742)]\n"
     ]
    }
   ],
   "source": [
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keyphrases = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', top_n=5)\n",
    "\n",
    "print(\"Top 5 Keyphrases:\", keyphrases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Max Sum Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases using Max Sum Similarity: [('world class tutor', 0.3111), ('months seeing headlines', 0.3114), ('canceling student debt', 0.325), ('totalitarian governments criminal', 0.3513), ('better math people', 0.365)]\n"
     ]
    }
   ],
   "source": [
    "# 2.3. Max Sum Similarity\n",
    "# To diversity the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n\n",
    "# words and extract the combination that are the least similar to each other by cosine similarity.\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keyphrases1 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(\"Top 5 Keyphrases using Max Sum Similarity:\", keyphrases1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases using Maximal Marginal Relevance: [('student debating cancel', 0.4364), ('totalitarian governments criminal', 0.3513), ('high school arizona', 0.1407), ('mississippi river brings', 0.107), ('intelligent amazing personal', 0.0978)]\n"
     ]
    }
   ],
   "source": [
    "# 2.4. Maximal Marginal Relevance\n",
    "# To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity.\n",
    "# The results with high diversity:\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keyphrases2 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7, top_n=5)\n",
    "print(\"Top 5 Keyphrases using Maximal Marginal Relevance:\", keyphrases2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases using Sentence Transformer: [('famous speech stanford', 0.299), ('teachers just saw', 0.3004), ('months seeing headlines', 0.3114), ('totalitarian governments criminal', 0.3513), ('better math people', 0.365)]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\", device=\"cpu\")\n",
    "model = KeyBERT(model=sentence_model)\n",
    "keyphrases3 =  model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=30, top_n=5)\n",
    "print(\"Top 5 Keyphrases using Sentence Transformer:\", keyphrases3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Spacy Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keyphrases using Spacy Transformer: [('tell mistake instead', 0.4454), ('life coach exactly', 0.448), ('socratically like oral', 0.4577), ('scaring accident ai', 0.4582), ('pessimistic view ai', 0.4835)]\n"
     ]
    }
   ],
   "source": [
    "# Using spacy-transformer models:\n",
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
    "model = KeyBERT(model=nlp)\n",
    "keyphrases4 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(\"Top 5 Keyphrases using Spacy Transformer:\", keyphrases4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
